---
title: "Data Analysis"
format: revealjs
editor: visual
---

```{r libraries}
library(tidyr)
library(ggplot2)
library(cs84040smo)
library(caret)
library(dplyr)
library(FSelector)
```

# Correlation Analysis

## Importing data

```{r read-data}
BASE_PATH <- file.path("~/Classes/cs84040/project")
DATA_SAVE_PATH <- file.path(BASE_PATH, "data")

final_labeled_data <- read.csv(file.path(DATA_SAVE_PATH, "final_labeled_data.csv"))
```

## Make target binary and remove unneeded columns

```{r correlation-analysis, message=FALSE}
labeled_data <- final_labeled_data |>
  dplyr::mutate(y = ifelse(Type_Phage == "Virulent", 1, -1),
                .after = Type_Phage) |>
  dplyr::select(-c(X, Meta, Type_Phage))

print(paste("No NAs?", all(!is.na(labeled_data))))

y <- labeled_data |>
    dplyr::select(c(y))
X <- labeled_data |>
    dplyr::select(-c(y))
```

## Explore data

We use `caret` and `FSelector` to remove features with near zero variance
predictors then select features with a high correlation.

```{r first-steps}
# Remove near zero variance features (reduces to 236 features)
nzv <- caret::nearZeroVar(X, saveMetrics = TRUE)
X_filtered <- X[, !nzv$nzv]

# Find highly correlated features to remove (reduce by 44 features)
high_cor <- caret::findCorrelation(cor(X_filtered), cutoff = 0.95)
X_filtered <- X_filtered[, -high_cor]

# Reduce to 174 by selecting features with higher gain (higher importance)
weights <- FSelector::information.gain(y ~ ., data = cbind(X_filtered, y))
sorted_weights <- sort(weights$attr_importance, decreasing = TRUE)
cumsum_importance <- cumsum(sorted_weights) / sum(sorted_weights)
n_features <- which(cumsum_importance >= 0.95)[1]

plot(cumsum_importance, type = "b",
     xlab = "Number of Features",
     ylab = "Cumulative Information Gain",
     main = "Feature Selection")
abline(h = 0.95, col = "red", lty = 2)
```

We select the top `n_features` (= 174) features with the highest information gain.

```{r select-top-features}
top_features <- cutoff.k(weights, k = n_features) # 174
X_selected <- X[, top_features]
```

## Initial PCA

```{r initial-plot}
pca <- prcomp(X_selected, scale. = TRUE)
df_pca <- data.frame(
    PC1 = pca$x[,1],
    PC2 = pca$x[,2],
    class = as.factor(y$y)
)

ggplot(df_pca, aes(x = PC1, y = PC2, color = class)) +
    geom_point(alpha = 0.5) +
    ggtitle("PCA after selecting features") +
    theme_minimal()
```

## Splitting Data

We use caret to split the preprocessed data into training, testing, and validation
sets.

```{r splitting-data}

set.seed(107)

in_training <- caret::createDataPartition(y = labeled_data$y, p = .80,
                                          list = FALSE)

X_train <- X_selected[in_training, ]
y_train <- y[in_training, ]

X_test <- X_selected[-in_training, ]
y_test <- y[-in_training, ]
```

# Skewedness, scaling

I suspected the data would be skewed when generating because the peptide
frequencies varied and were vary sparse in comparison to biological data
generated by BioPython.

```{r}
library(e1071)

skewness_values <- apply(X_train, 2, function(x) {
  skewness(x, na.rm = TRUE)
})
highly_skewed <- abs(skewness_values) > 1
highly_skewed_features <- names(X_train)[highly_skewed]
```

```{r plotting-skewed-data}
X_train[highly_skewed_features[1:2]] |>
    tidyr::pivot_longer(everything()) |>
    ggplot2::ggplot(aes(x = value)) +
    ggplot2::geom_histogram() +
    ggplot2::facet_wrap(~name, scales = "free") +
    ggplot2::theme_minimal()

```

We'll apply a transformation on highly skewed features to reduce
the skew with `caret`.

```{r yeo-johnson}
trans <- caret::preProcess(X_train[, highly_skewed_features],
                           method = c("YeoJohnson", "center", "scale"))
X_train_transformed <- X_train
X_train_transformed[, highly_skewed_features] <- predict(trans,
                                                         X_train[, highly_skewed_features])
X_test_transformed <- predict(trans, X_test)
```

## Plotting 2 features after transformation

```{r plotting-transformed-skewed-data}
X_train_transformed[highly_skewed_features[1:2]] |>
    tidyr::pivot_longer(everything()) |>
    ggplot2::ggplot(aes(x = value)) +
    ggplot2::geom_histogram() +
    ggplot2::facet_wrap(~name, scales = "free") +
    theme_minimal()
```

## Downsampling to deal with very biased data

```{r unbalanced}
ys <- table(y)
print(paste("Temperate phage observations are", 100 * ys[1] / sum(ys[2]),
            "% of the data."))
```

To deal with this, we can either downsample or use a package like SMOTE to
generate synthetic data. Since there's a lot of positive samples, we'll try
downSample with `caret` to reduce the major class.

```{r downsampling}
X_train_down <- caret::downSample(X_train_transformed, factor(y_train))
y_train_balanced <- as.numeric(as.character(X_train_down$Class))
X_train_balanced <- X_train_down[, -ncol(X_train_down)]
y_balanced <- table(y_train_balanced)
print(paste("Temperate phage observations are",
            100 * y_balanced[1] / sum(y_balanced), "% of 720 observations."))
```

## Second PCA

```{r second-plot}
pca <- prcomp(X_train_balanced, scale. = TRUE)
df_pca <- data.frame(
    PC1 = pca$x[,1],
    PC2 = pca$x[,2],
    class = as.factor(y_train_balanced)
)

ggplot(df_pca, aes(x = PC1, y = PC2, color = class)) +
    geom_point(alpha = 0.5) +
    ggtitle("PCA after transformations") +
    theme_minimal()
```

```{r training-balanced}
# Yeo Johnson transformation to make more normal
# Reduced features - removed zero variance, selected those with high correlation
# Reduced skew
# 80/20 split
# max_iterations: 1000

model_balanced <- SmoModel$new(x = X_train_balanced, y = y_train_balanced,
                               kernel = "gaussian", verbose = TRUE, C = 5,
                               sigma = 40.0, tol = 0.001)
fit_smo(model_balanced, max_iterations = 1000)
saveRDS(model_balanced, file = "~/Classes/cs84040/project/code/smo/inst/trained_models/balanced_360x360_feat174_training")
#readRDS(model_balanced)
model_balanced

# Training
mb_train_predict <- model_balanced$predict(X_train_balanced)
mb_cm_train  <- confusionMatrix(as.factor(y_train_balanced),
                                as.factor(mb_train_predict),
                                positive = "1")

# Testing
mb_test_predict <- model_balanced$predict(X_test_transformed)
mb_cm_test <- confusionMatrix(factor(y_test), 
                              factor(mb_test_predict),
                              positive = "1")
```

```{r cm-plot-training}
# Generated by claude

# Convert to data frame for ggplot
cm_df <- as.data.frame(mb_cm_train$table)
names(cm_df) <- c("Prediction", "Reference", "Count")

# Calculate percentages
cm_df$Percentage <- cm_df$Count / sum(cm_df$Count) * 100

# Add labels for TN, FP, FN, TP
cm_df$Type <- c("TN", "FP", "FN", "TP")

# Add display label
cm_df$Label <- sprintf("%s\n%d\n(%.1f%%)", cm_df$Type, cm_df$Count, cm_df$Percentage)

# Determine if correct or incorrect
cm_df$Correct <- ifelse(cm_df$Prediction == cm_df$Reference, "Correct", "Incorrect")

# Create plot
ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Correct)) +
    geom_tile(color = "white", size = 2) +
    geom_text(aes(label = Label), 
              size = 5,
              fontface = "bold",
              color = "white") +
    scale_fill_manual(values = c("Correct" = "#d88c9a", 
                                  "Incorrect" = "#99c1b9")) +
    labs(
        title = "(Training) Confusion Matrix",
        subtitle = sprintf(
            "Accuracy: %.1f%% | Balanced Acc: %.1f%%",
            mb_cm_train$overall['Accuracy'] * 100,
            mb_cm_train$byClass['Balanced Accuracy'] * 100
        ),
        x = "Actual Class",
        y = "Predicted Class"
    ) +
    theme_minimal(base_size = 14) +
    theme(
        plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 11),
        axis.text = element_text(size = 13, face = "bold"),
        legend.position = "bottom",
        legend.title = element_blank()
    ) +
    coord_fixed()
```
```{r cm-plot-testing}
# Generated by claude

# Convert to data frame for ggplot
cm_df <- as.data.frame(mb_cm_test$table)
names(cm_df) <- c("Prediction", "Reference", "Count")

# Calculate percentages
cm_df$Percentage <- cm_df$Count / sum(cm_df$Count) * 100

# Add labels for TN, FP, FN, TP
cm_df$Type <- c("TN", "FP", "FN", "TP")

# Add display label
cm_df$Label <- sprintf("%s\n%d\n(%.1f%%)", cm_df$Type, cm_df$Count, cm_df$Percentage)

# Determine if correct or incorrect
cm_df$Correct <- ifelse(cm_df$Prediction == cm_df$Reference, "Correct", "Incorrect")

# Create plot
ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Correct)) +
    geom_tile(color = "white", size = 2) +
    geom_text(aes(label = Label), 
              size = 5,
              fontface = "bold",
              color = "white") +
    scale_fill_manual(values = c("Correct" = "#d88c9a", 
                                  "Incorrect" = "#99c1b9")) +
    labs(
        title = "(Testing) Confusion Matrix",
        subtitle = sprintf(
            "Accuracy: %.1f%% | Balanced Acc: %.1f%%",
            mb_cm_train$overall['Accuracy'] * 100,
            mb_cm_train$byClass['Balanced Accuracy'] * 100
        ),
        x = "Actual Class",
        y = "Predicted Class"
    ) +
    theme_minimal(base_size = 14) +
    theme(
        plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 11),
        axis.text = element_text(size = 13, face = "bold"),
        legend.position = "bottom",
        legend.title = element_blank()
    ) +
    coord_fixed()
```

```{r testing-for-different-cs}
# Generated by claud

# Test different C values with sigma=40
c_values <- c(0.01, 0.1, 0.5, 1, 5, 10)

for (c_val in c_values) {
    cat(sprintf("\n=== C = %.2f ===\n", c_val))
    model <- SmoModel$new(
        x = X_train_balanced,
        y = y_train_balanced,
        kernel = "gaussian",
        sigma = 40,
        C = c_val,
        tol = 0.001,
        verbose = FALSE
    )
    
    model <- fit_smo(model, max_iterations = 1000)
    
    n_sv <- sum(model$alpha > 1e-8)
    at_C <- sum(abs(model$alpha - model$C) < 1e-8)
    
    preds <- model$predict(X_train_balanced)
    acc <- mean(preds == y_train_balanced)
    
    cat(sprintf("SVs: %3d, At C: %3d, Acc: %.3f, Alpha range: [%.4f, %.4f]\n",
                n_sv, at_C, acc, min(model$alpha), max(model$alpha)))
}
```


```{r sessionInfo}
sessionInfo()
```